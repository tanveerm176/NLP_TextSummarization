{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d68OzNISzddi"
      },
      "source": [
        "# Import and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJSN45Ul9a3G"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "pd.options.display.max_colwidth = 150\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jDig9S1BDZ8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "qHMtbuRLCMIe"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# WikiHow\n",
        "# wikihow = pd.read_csv(\"wikihowAll.csv\")\n",
        "wikihow = pd.read_csv(\"/content/drive/Othercomputers/MacBook Air 2021/Desktop/Hunter/NLP/Project/data/wikihowAll.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHrHXMNjCMIf"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# CNN-Dailymail - Test\n",
        "# cnn_daily_test = pd.read_csv(\"cnn_dailymail/test.csv\")\n",
        "cnn_daily_test = pd.read_csv(\"/content/drive/Othercomputers/MacBook Air 2021/Desktop/Hunter/NLP/Project/data/cnn_dailymail/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNIwGQCmCMIf"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# CNN-Dailymail - Validation\n",
        "# cnn_daily_valid = pd.read_csv(\"cnn_dailymail/validation.csv\")\n",
        "cnn_daily_valid = pd.read_csv(\"/content/drive/Othercomputers/MacBook Air 2021/Desktop/Hunter/NLP/Project/data/cnn_dailymail/validation.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-2MjSbX2CMIg"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# CNN-Dailymail - Train\n",
        "# cnn_daily_train = pd.read_csv(\"cnn_dailymail/train.csv\")\n",
        "cnn_daily_train = pd.read_csv(\"/content/drive/Othercomputers/MacBook Air 2021/Desktop/Hunter/NLP/Project/data/cnn_dailymail/train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7H5sDbFCMIg"
      },
      "outputs": [],
      "source": [
        "cnn_daily = pd.concat([cnn_daily_train, cnn_daily_valid, cnn_daily_test]).reset_index(drop=True)\n",
        "cnn_daily = cnn_daily[[\"article\", \"highlights\"]].rename(columns={\"highlights\":\"summary\"})\n",
        "cnn_daily = cnn_daily.replace(r'\\n',' ', regex=True)\n",
        "cnn_daily = cnn_daily.replace(r'\\s+([.,;:!?])', r'\\1', regex=True)\n",
        "cnn_daily = cnn_daily.astype(str)\n",
        "\n",
        "arr_filter = cnn_daily[\"article\"].apply(lambda x: len(x)) > 400\n",
        "\n",
        "cnn_daily = cnn_daily.loc[arr_filter, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "EreZDF_jCMIg"
      },
      "outputs": [],
      "source": [
        "cnn_daily.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42zq5ukhCMIh"
      },
      "outputs": [],
      "source": [
        "wikihow_clean = wikihow[[\"headline\", \"text\"]].rename(columns={\"headline\":\"summary\", \"text\":\"article\"})\n",
        "wikihow_clean = wikihow_clean.replace(r'\\n',' ', regex=True)\n",
        "wikihow_clean = wikihow_clean.replace(r'\\s+([.,;:!?])', r'\\1', regex=True)\n",
        "\n",
        "wikihow_clean = wikihow_clean.astype(str)\n",
        "\n",
        "arr_filter = wikihow_clean[\"article\"].apply(lambda x: len(x)) > 400\n",
        "\n",
        "wikihow_clean = wikihow_clean.loc[arr_filter, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "kD1yFUFFCMIh"
      },
      "outputs": [],
      "source": [
        "wikihow_clean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaLNCb8nCMIh"
      },
      "outputs": [],
      "source": [
        "print(cnn_daily.shape)\n",
        "print(wikihow_clean.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarization Methods"
      ],
      "metadata": {
        "id": "g9kGRYDkf-M1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and load models (PEGASUS and BERTSUM)"
      ],
      "metadata": {
        "id": "vgUV5At6qGP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Install transformers library\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "# nltk --> for ensemble summarizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ci_xwhVVgGRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Import relevant libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, BertTokenizer, EncoderDecoderModel\n",
        "from transformers import pipeline, PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import heapq\n",
        "import networkx as nx\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "# Extractive summarization model and tokenizer (BERTSUM)\n",
        "extractive_model = EncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
        "extractive_tokenizer = BertTokenizer.from_pretrained(\"patrickvonplaten/bert2bert_cnn_daily_mail\")\n",
        "\n",
        "# Abstractive summarization pipeline (PEGASUS-XSUM)\n",
        "abstractive_summarizer = pipeline(\"summarization\", model=\"google/pegasus-xsum\")\n",
        "\n",
        "# Abstractive summarization model and tokenizer (PEGASUS-XSUM)\n",
        "abstractive_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\")\n",
        "abstractive_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\")\n",
        "\n",
        "# Get single example (for testing methods)\n",
        "cnn_example_summary = cnn_daily[\"summary\"][2]\n",
        "cnn_example_text = cnn_daily[\"article\"][2]"
      ],
      "metadata": {
        "id": "F4b9o4N_gILJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline"
      ],
      "metadata": {
        "id": "taJkY2XsD8-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline, with k=4\n",
        "def baseline(text):\n",
        "    sentences = re.split(r'(?<=[.:;!?])\\s', text)\n",
        "    selected_sentences = [sentence for sentence in sentences if len(sentence.split()) >= 3][:4]\n",
        "\n",
        "    return(''.join(selected_sentences))\n",
        "\n",
        "print(f\"Reference Sumamry:\\n{cnn_example_summary}\\n\")\n",
        "print(f\"Baseline:\\n{baseline(cnn_example_text)}\\n\")"
      ],
      "metadata": {
        "id": "WE0gh_ExDtHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base abstractive and extractive summarization"
      ],
      "metadata": {
        "id": "wYltT8fD4l2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base function for abstractive summarization (PEGASUS)\n",
        "def abstractive_summarization(text,\n",
        "                              max_length=None, min_length=None, do_sample=False,\n",
        "                              truncate = True,\n",
        "                              summarizer=abstractive_summarizer):\n",
        "\n",
        "    max_input_length=1024\n",
        "    # Truncate input text if its length exceeds `max_input_length`\n",
        "    if truncate and len(text) > max_input_length:\n",
        "        text = text[:max_input_length]\n",
        "        # Find the last complete sentence before the truncated point\n",
        "        last_period_idx = text.rfind(\".\")\n",
        "        if last_period_idx != -1:\n",
        "            text = text[:last_period_idx+1]\n",
        "\n",
        "    tokens = abstractive_tokenizer.tokenize(text)\n",
        "\n",
        "    if max_length is None:\n",
        "        max_length = len(tokens) // 2\n",
        "\n",
        "    if min_length is None:\n",
        "        min_length = len(tokens) // 4\n",
        "\n",
        "    summary = summarizer(text, max_length=max_length, min_length=min_length, do_sample=do_sample)\n",
        "    return summary[0][\"summary_text\"]\n",
        "\n",
        "\n",
        "# Base function for extractive summarization (BERTSUM)\n",
        "def extractive_summarization(text,\n",
        "                             padding=True, truncation=True,\n",
        "                             num_beams=5, max_length=None,\n",
        "                             early_stopping=True, skip_special_tokens=True,\n",
        "                             model=extractive_model, tokenizer=extractive_tokenizer):\n",
        "\n",
        "    if max_length is None:\n",
        "        max_length = len(text) // 7\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=padding, truncation=truncation)\n",
        "\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=num_beams, max_length=max_length,\n",
        "                                 early_stopping=early_stopping)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=skip_special_tokens)\n",
        "    return summary\n",
        "\n",
        "print(f\"Reference Sumamry:\\n{cnn_example_summary}\\n\")\n",
        "\n",
        "print(f\"Abstractive Summarization:\\n{abstractive_summarization(cnn_example_text)}\\n\")\n",
        "print(f\"Extractive Summarization:\\n{extractive_summarization(cnn_example_text)}\\n\")"
      ],
      "metadata": {
        "id": "9985gK4GF0Hl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c600c8-633c-4985-ecd1-422a9af9f575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference Sumamry:\n",
            "Craig Eccleston-Todd, 27, had drunk at least three pints before driving car. Was using phone when he veered across road in Yarmouth, Isle of Wight. Crashed head-on into 28-year-old Rachel Titley's car, who died in hospital. Police say he would have been over legal drink-drive limit at time of crash. He was found guilty at Portsmouth Crown Court of causing death by dangerous driving.\n",
            "\n",
            "Abstractive Summarization:\n",
            "A drunk driver who killed a young woman in a head-on crash while checking his mobile phone has been jailed for six years, reports the BBC's Victoria Derbyshire programme, which is broadcast on BBC One in the Isle of Wight and on BBC Two in the South East.\n",
            "\n",
            "Extractive Summarization:\n",
            "craig eccleston - todd, 27, was driving home from a pub when he crashed. he veered across the road and smashed into rachel titley's car coming the other way. miss titley was driving responsibly and there was'nothing she could have done to avoid'he was found guilty of causing death by dangerous driving at portsmouth crown court.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple two-step hybrid summarization"
      ],
      "metadata": {
        "id": "11yFKQkz4p3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Hybrid extractive-abstractive (simple two step approach)\n",
        "\n",
        "In this example, we will create a hybrid extractive-abstractive summarizer using BERTSUM (extractive)\n",
        "and PEGASUS (abstractive). The idea is to first generate an extractive summary using BERTSUM,\n",
        "which will serve as a condensed version of the original text, and then use PEGASUS to create an\n",
        "abstractive summary from the extractive summary. This two-step approach combines the strengths of both\n",
        "extractive and abstractive methods.\n",
        "'''\n",
        "def hybrid_summarization(text, num_steps=3):\n",
        "\n",
        "    # Step 1: Split the text into equal parts\n",
        "    step_size = len(text) // num_steps\n",
        "    text_parts = [text[i:i+step_size] for i in range(0, len(text), step_size)]\n",
        "\n",
        "    # Step 2: Perform extractive summarization on each part\n",
        "    extractive_summaries = []\n",
        "    for part in text_parts:\n",
        "        extractive_summary = extractive_summarization(part, max_length=step_size)\n",
        "        extractive_summaries.append(extractive_summary)\n",
        "\n",
        "    # Step 3: Concatenate the extractive summaries and perform abstractive summarization\n",
        "    abstractive_summary = abstractive_summarization(\" \".join(extractive_summaries))\n",
        "\n",
        "    return abstractive_summary\n",
        "\n",
        "\n",
        "print(f\"Reference Sumamry:\\n{cnn_example_summary}\\n\")\n",
        "\n",
        "print(f\"Hybrid (two-step) Summarization:\\n{hybrid_summarization(cnn_example_text)}\\n\")"
      ],
      "metadata": {
        "id": "U4_n_sPrG5ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a11a81-f1df-4981-ea55-37e1aa27285e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference Sumamry:\n",
            "Craig Eccleston-Todd, 27, had drunk at least three pints before driving car. Was using phone when he veered across road in Yarmouth, Isle of Wight. Crashed head-on into 28-year-old Rachel Titley's car, who died in hospital. Police say he would have been over legal drink-drive limit at time of crash. He was found guilty at Portsmouth Crown Court of causing death by dangerous driving.\n",
            "\n",
            "Hybrid (two-step) Summarization:\n",
            "A man has been jailed for six years for causing the death of a woman who was killed when he crashed into her car as he was reading or replying to a text message, a court has been told. eccleston - todd, of newport, was found guilty of causing death by dangerous driving following trial at portsmouth crown court.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two-step hybrid summarization with 'importance ranking' in extractive stage"
      ],
      "metadata": {
        "id": "Q_yQgKfM4xtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Hybrid extractive-abstractive (two step approach, this time focusing on \"most important sentences\" in extractive stage)\n",
        "\n",
        "In this approach, we will first extract the most important sentences, measured by length from summary,\n",
        "from the input text using an extractive summarizer,\n",
        "then use these sentences as input for the abstractive summarization model.\n",
        "This is similar to the previous hybrid approach but prioritizes the \"most important sentences\" (proxied by length)\n",
        "from the extractive summary.\n",
        "'''\n",
        "\n",
        "def extractive_summarization_priority(text, num_sentences=20,\n",
        "                                      padding=True, truncation=True,\n",
        "                                      num_beams=5, max_length=200,\n",
        "                                      early_stopping=False, skip_special_tokens=True,\n",
        "                                      model=extractive_model, tokenizer=extractive_tokenizer):\n",
        "\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=padding, truncation=truncation)\n",
        "    summary_ids = model.generate(inputs[\"input_ids\"], num_beams=num_beams, max_length=len(text),\n",
        "                                 early_stopping=early_stopping)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=skip_special_tokens)\n",
        "\n",
        "    # Extract the most important sentences, measured by length\n",
        "    sentences = summary.split('. ')\n",
        "    top_sentences = heapq.nlargest(num_sentences, sentences, key=len)\n",
        "    important_sentences = '. '.join(top_sentences)\n",
        "\n",
        "    return important_sentences\n",
        "\n",
        "def hybrid_summarization_important_sentences(text):\n",
        "    # Step 1: Extractive summarization, with important sentence selection\n",
        "    important_sentences = extractive_summarization_priority(text)\n",
        "\n",
        "    # Step 2: Abstractive summarization\n",
        "    abstractive_summary = abstractive_summarization(important_sentences)\n",
        "\n",
        "    return abstractive_summary\n",
        "\n",
        "print(f\"Reference Sumamry:\\n{cnn_example_text}\\n\")\n",
        "\n",
        "print(f\"Hybrid (important sentences) Summarization:\\n{hybrid_summarization_important_sentences(cnn_example_text)}\\n\")"
      ],
      "metadata": {
        "id": "N1Unv8k-Hsop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea013b83-4fa8-44f1-f106-62a8ab074d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference Sumamry:\n",
            "A drunk driver who killed a young woman in a head-on crash while checking his mobile phone has been jailed for six years. Craig Eccleston-Todd, 27, was driving home from a night at a pub when he received a text message. As he was reading or replying to it, he veered across the road while driving round a bend and smashed into Rachel Titley’s car coming the other way. Craig Eccleston-Todd, 27 (left) was using his mobile phone when he crashed head-on into the car being driven by Rachel Titley, 28 (right). She died later from her injuries. The head-on crash took place in October 2013. Mr Eccleston-Todd's car was barely recognisable (pictured) Police said Eccleston-Todd had drunk at least three or four pints of beer before getting behind the wheel. He was found guilty of causing death by dangerous driving at Portsmouth Crown Court yesterday. Miss Titley, a 28-year-old solicitor’s clerk from Cowes, Isle of Wight, had also spent the evening with friends at a pub but had not drunk any alcohol, police said. She was driving responsibly and there was ‘nothing she could have done to avoid the collision’, they added. Lindsay Pennell, prosecuting, said: ‘Craig Eccleston-Todd’s driving resulted in the tragic death of a young woman, Rachel Titley, a death that could have been avoided. ‘Mr Eccleston-Todd took the decision to pick up his mobile phone whilst driving and, either reading or replying to this text message, was so distracted that he failed to negotiate a left-hand bend, crossing the central white line into the path of Miss Titley’s oncoming car. Miss Titley was pulled the wreckage of her Daihatsu Cuore but died later from her injuries in hospital. ‘Miss Titley [had] a bright future ahead of her. She was also returning home having spent an enjoyable evening with friends and was driving responsibly. ‘She had arranged to contact her friends when she got home to confirm that she had arrived safely. Her friends sadly never heard from her after they parted company. ‘Miss Titley’s death in these circumstances reiterates the danger of using a hand-held mobile phone whilst driving.’ Police were unable to take breath or blood tests from Eccleston-Todd immediately, but in tests several hours after the accident he was only marginally under the drink-drive limit. The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titley’s blue Daihatsu Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013. His phone records showed he was also texting around the time of the crash. PC Mark Furse, from Hampshire constabulary’s serious collision investigation unit, said: 'Our thoughts are with Rachel's family at this time. She had been out with friends at a pub in Shalfleet that evening, but had not had any alcohol. 'Our investigation showed that there was nothing she could have done to avoid the collision and sadly it cost her her life. 'Mr Eccleston-Todd had left work in Yarmouth and met with friends at a pub where he drank at least three to four pints of lager. He hadn't long left the pub to return home when the collision occurred at around 9.30pm. 'We weren't able to take breath or blood tests from him immediately and although blood taken several hours after the collision showed he was marginally under the limit, we maintain he would have been over the limit at the time of the collision and in summing up today, the judge agreed. 'The analysis of his phone records showed that he was texting on his phone around the time of the collision so it's highly likely this would also have contributed to his dangerous driving and loss of control.' Eccleston-Todd was found guilty of causing death by dangerous driving following a trial at Portsmouth Crown Court (pictured) He added: 'Mr Eccleston-Todd will now spend six years behind bars, but Rachel's family have lost her forever. 'I hope this will make people think twice before drinking any alcohol and getting behind the wheel, or using a phone once they're on the road. 'The dangers of drink driving and driving whilst using a mobile phone are obvious. Those who continue to do so risk spending a substantial time in prison. This case highlights just how tragic the consequences of committing these offences can be.' ‘Mr Eccleston-Todd will now spend six years behind bars, but Rachel’s family have lost her for ever. I hope this will make people think twice before drinking any alcohol and getting behind the wheel, or using a phone once they’re on the road. This case highlights just how tragic the consequences of committing these offences can be.’ Eccleston-Todd, of Newport, Isle of Wight, was also disqualified from driving for eight years after which he will have to complete an extended re-test.\n",
            "\n",
            "Hybrid (important sentences) Summarization:\n",
            "A woman has been jailed for three years for causing the death of a man in a crash in Portsmouth.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph-based summarization, using TextRank"
      ],
      "metadata": {
        "id": "2Y2WuLDZsCQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Graph based summarization\n",
        "'''\n",
        "\n",
        "def build_similarity_matrix(sentences):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
        "    similarity_matrix = cosine_similarity(sentence_vectors)\n",
        "    return similarity_matrix\n",
        "\n",
        "def textrank(sentences, top_n=5):\n",
        "    similarity_matrix = build_similarity_matrix(sentences)\n",
        "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "    top_sentences = [ranked_sentences[i][1] for i in range(top_n)]\n",
        "    return '. '.join(top_sentences)\n",
        "\n",
        "def graph_based_summarization(text, top_n=10):\n",
        "    sentences = text.split('. ')\n",
        "    top_ranked_sentences = textrank(sentences, top_n)\n",
        "\n",
        "    summary = abstractive_summarization(top_ranked_sentences,\n",
        "                                        truncate=True,\n",
        "                                        do_sample=True)\n",
        "    return summary\n",
        "\n",
        "\n",
        "print(f\"Graph-based Summarization:\\n{graph_based_summarization(cnn_example_text)}\\n\")"
      ],
      "metadata": {
        "id": "G8MSOIBar32d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10635d00-edc1-4b9b-d268-1d4ca319ea87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph-based Summarization:\n",
            "A driver has been jailed for six years for causing the death of 19-year-old Rachel Titley, who was killed when she was hit by a car driven by Daniel Eccleston-Todd on the A3 in Portsmouth in March last year, police have said.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble hybrid approach, ranking results of models to generate final summary"
      ],
      "metadata": {
        "id": "yLMuUQ0ZsE-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Ensemble approach\n",
        "\n",
        "'''\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "\n",
        "    sent1 = [w.lower() for w in sent1 if w not in stopwords]\n",
        "    sent2 = [w.lower() for w in sent2 if w not in stopwords]\n",
        "\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "\n",
        "    for w in sent1:\n",
        "        vector1[all_words.index(w)] += 1\n",
        "\n",
        "    for w in sent2:\n",
        "        vector2[all_words.index(w)] += 1\n",
        "\n",
        "    return 1 - cosine_similarity(np.array(vector1).reshape(1, -1), np.array(vector2).reshape(1, -1))[0, 0]\n",
        "\n",
        "def build_similarity_matrix(sentences, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "\n",
        "    S = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        for j in range(len(sentences)):\n",
        "            if i == j:\n",
        "                continue\n",
        "\n",
        "            S[i][j] = sentence_similarity(sentences[i], sentences[j], stopwords)\n",
        "\n",
        "    for i in range(len(S)):\n",
        "        S[i] /= S[i].sum()\n",
        "\n",
        "    return S\n",
        "\n",
        "def ensemble_summarization(extractive_summary, abstractive_summary, top_n=5):\n",
        "\n",
        "    # Extract sentences\n",
        "    sentences = list(set(nltk.sent_tokenize(extractive_summary) + nltk.sent_tokenize(abstractive_summary)))\n",
        "\n",
        "    # Build similarity matrix\n",
        "    S = build_similarity_matrix(sentences, stopwords.words(\"english\"))\n",
        "\n",
        "    # Rank sentences\n",
        "    ranked_sentences = nx.pagerank(nx.from_numpy_array(S), alpha=0.85, tol=1e-8)\n",
        "\n",
        "    # Extract the top-ranked sentences\n",
        "    top_sentences = heapq.nlargest(top_n, ranked_sentences, key=ranked_sentences.get)\n",
        "    summary = \". \".join([sentences[i] for i in top_sentences])\n",
        "\n",
        "    return summary\n",
        "\n",
        "print(f\"Reference Sumamry:\\n{cnn_example_text}\\n\")\n",
        "\n",
        "extractive_summary = extractive_summarization(cnn_example_text)\n",
        "abstractive_summary = abstractive_summarization(cnn_example_text)\n",
        "print(f\"Hybrid (ensemble, using TextRank to rank) Summarization:\\n{ensemble_summarization(extractive_summary, abstractive_summary)}\\n\")"
      ],
      "metadata": {
        "id": "BJvcCBLgH5PR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b93ccf3-f343-44dd-c3fa-14795af81cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference Sumamry:\n",
            "A drunk driver who killed a young woman in a head-on crash while checking his mobile phone has been jailed for six years. Craig Eccleston-Todd, 27, was driving home from a night at a pub when he received a text message. As he was reading or replying to it, he veered across the road while driving round a bend and smashed into Rachel Titley’s car coming the other way. Craig Eccleston-Todd, 27 (left) was using his mobile phone when he crashed head-on into the car being driven by Rachel Titley, 28 (right). She died later from her injuries. The head-on crash took place in October 2013. Mr Eccleston-Todd's car was barely recognisable (pictured) Police said Eccleston-Todd had drunk at least three or four pints of beer before getting behind the wheel. He was found guilty of causing death by dangerous driving at Portsmouth Crown Court yesterday. Miss Titley, a 28-year-old solicitor’s clerk from Cowes, Isle of Wight, had also spent the evening with friends at a pub but had not drunk any alcohol, police said. She was driving responsibly and there was ‘nothing she could have done to avoid the collision’, they added. Lindsay Pennell, prosecuting, said: ‘Craig Eccleston-Todd’s driving resulted in the tragic death of a young woman, Rachel Titley, a death that could have been avoided. ‘Mr Eccleston-Todd took the decision to pick up his mobile phone whilst driving and, either reading or replying to this text message, was so distracted that he failed to negotiate a left-hand bend, crossing the central white line into the path of Miss Titley’s oncoming car. Miss Titley was pulled the wreckage of her Daihatsu Cuore but died later from her injuries in hospital. ‘Miss Titley [had] a bright future ahead of her. She was also returning home having spent an enjoyable evening with friends and was driving responsibly. ‘She had arranged to contact her friends when she got home to confirm that she had arrived safely. Her friends sadly never heard from her after they parted company. ‘Miss Titley’s death in these circumstances reiterates the danger of using a hand-held mobile phone whilst driving.’ Police were unable to take breath or blood tests from Eccleston-Todd immediately, but in tests several hours after the accident he was only marginally under the drink-drive limit. The judge agreed with police that he would have been over the limit at the time his red Citroen hit Miss Titley’s blue Daihatsu Cuore on a road near Yarmouth, Isle of Wight, on October 11, 2013. His phone records showed he was also texting around the time of the crash. PC Mark Furse, from Hampshire constabulary’s serious collision investigation unit, said: 'Our thoughts are with Rachel's family at this time. She had been out with friends at a pub in Shalfleet that evening, but had not had any alcohol. 'Our investigation showed that there was nothing she could have done to avoid the collision and sadly it cost her her life. 'Mr Eccleston-Todd had left work in Yarmouth and met with friends at a pub where he drank at least three to four pints of lager. He hadn't long left the pub to return home when the collision occurred at around 9.30pm. 'We weren't able to take breath or blood tests from him immediately and although blood taken several hours after the collision showed he was marginally under the limit, we maintain he would have been over the limit at the time of the collision and in summing up today, the judge agreed. 'The analysis of his phone records showed that he was texting on his phone around the time of the collision so it's highly likely this would also have contributed to his dangerous driving and loss of control.' Eccleston-Todd was found guilty of causing death by dangerous driving following a trial at Portsmouth Crown Court (pictured) He added: 'Mr Eccleston-Todd will now spend six years behind bars, but Rachel's family have lost her forever. 'I hope this will make people think twice before drinking any alcohol and getting behind the wheel, or using a phone once they're on the road. 'The dangers of drink driving and driving whilst using a mobile phone are obvious. Those who continue to do so risk spending a substantial time in prison. This case highlights just how tragic the consequences of committing these offences can be.' ‘Mr Eccleston-Todd will now spend six years behind bars, but Rachel’s family have lost her for ever. I hope this will make people think twice before drinking any alcohol and getting behind the wheel, or using a phone once they’re on the road. This case highlights just how tragic the consequences of committing these offences can be.’ Eccleston-Todd, of Newport, Isle of Wight, was also disqualified from driving for eight years after which he will have to complete an extended re-test.\n",
            "\n",
            "Hybrid (ensemble, using TextRank to rank) Summarization:\n",
            "miss titley was driving responsibly and there was'nothing she could have done to avoid'he was found guilty of causing death by dangerous driving at portsmouth crown court.. he veered across the road and smashed into rachel titley's car coming the other way.. craig eccleston - todd, 27, was driving home from a pub when he crashed.. A drunk driver who killed a young woman in a head-on crash while checking his mobile phone has been jailed for six years, reports the BBC's Victoria Derbyshire programme, which is broadcast on BBC One in the Isle of Wight and on BBC Two in the South East.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Heirarchical summarization, primarily for WikiHow (will need better preprocessing)"
      ],
      "metadata": {
        "id": "fBPXnR-45Co0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Heirarchical Summarization (for WikiHow primarily)\n",
        "'''\n",
        "\n",
        "def split_text_into_sections(text, delimiter='\\n\\n'):\n",
        "    sections = text.split(delimiter)\n",
        "    return [section.strip() for section in sections if section.strip()]\n",
        "\n",
        "def hierarchical_abstractive_summarization(text):\n",
        "    sections = split_text_into_sections(text)\n",
        "    section_summaries = [abstractive_summarization(section, min_length=1, max_length=min(len(section.split()), 25)) for section in sections]\n",
        "    summary_of_summaries = abstractive_summarization(' '.join(section_summaries))\n",
        "    return summary_of_summaries\n",
        "\n",
        "\n",
        "print(f\"Hierarchical Abstractive Summarization:\\n{hierarchical_abstractive_summarization(wikihow['text'][0])}\\n\")\n"
      ],
      "metadata": {
        "id": "GgVlNdpPJUIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c662e4-bed8-4824-dba0-88e53e161a3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hierarchical Abstractive Summarization:\n",
            "Whether you're making art for a living or just daydreaming, here are some tips for getting the most out of , as visual people, a lot of artist clutter comes from a desire to keep track of supplies visually instead of\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iterative abstractive summarization --> perhaps for longer texts"
      ],
      "metadata": {
        "id": "dB3WpoL55Jn9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Iterative summarization\n",
        "'''\n",
        "\n",
        "def merge_similar_sentences(text, similarity_threshold=0.8):\n",
        "    sentences = text.split('. ')\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    sentence_vectors = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    merged_sentences = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        if i == len(sentences) - 1:\n",
        "            break\n",
        "\n",
        "        similarity = cosine_similarity(sentence_vectors[i], sentence_vectors[i + 1])\n",
        "        if similarity > similarity_threshold:\n",
        "            merged_sentences.append(sentence + \" \" + sentences[i + 1])\n",
        "        else:\n",
        "            merged_sentences.append(sentence)\n",
        "\n",
        "    return '. '.join(merged_sentences)\n",
        "\n",
        "def remove_short_sentences(text, length_threshold=5):\n",
        "    sentences = text.split('. ')\n",
        "    long_sentences = [sentence for sentence in sentences if len(sentence.split()) > length_threshold]\n",
        "    return '. '.join(long_sentences)\n",
        "\n",
        "def iterative_abstractive_summarization(text, iterations=3):\n",
        "    current_summary = text\n",
        "    for _ in range(iterations):\n",
        "        current_summary = abstractive_summarization(current_summary, min_length=60, max_length=100)\n",
        "        current_summary = merge_similar_sentences(current_summary)\n",
        "        current_summary = remove_short_sentences(current_summary)\n",
        "    return current_summary\n",
        "\n",
        "print(f\"Iterative Abstractive Summarization:\\n{iterative_abstractive_summarization(cnn_example_text)}\\n\")"
      ],
      "metadata": {
        "id": "6UtEkKcAJYQB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910b57f8-28f1-45a0-8d28-c6ce9472ed63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 100, but you input_length is only 66. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterative Abstractive Summarization:\n",
            "Health officials in North Dakota say hundreds of church members may have been exposed to hepatitis A after a Catholic bishop was diagnosed with the virus in Italy health officials in North Dakota say hundreds of church members may have been exposed to hepatitis A after a Catholic bishop was diagnosed with the virus in Italy health officials in North Dakota say hundreds of church members may have been exposed to hepatitis A after a Catholic bishop was diagnosed with the virus in Italy.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query-based summarization (information retreival purposes)"
      ],
      "metadata": {
        "id": "eatwgDdJ5ct3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Query-based summarization\n",
        "'''\n",
        "\n",
        "def retrieve_relevant_sentences(text, query, num_sentences=5):\n",
        "    sentences = text.split('. ')\n",
        "    all_text = [query] + sentences\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(all_text)\n",
        "\n",
        "    query_vector = tfidf_matrix[0]\n",
        "    sentence_vectors = tfidf_matrix[1:]\n",
        "\n",
        "    similarities = cosine_similarity(query_vector, sentence_vectors)\n",
        "    top_sentence_indices = heapq.nlargest(num_sentences, range(len(similarities[0])), similarities[0].__getitem__)\n",
        "    relevant_sentences = [sentences[index] for index in sorted(top_sentence_indices)]\n",
        "    return '. '.join(relevant_sentences)\n",
        "\n",
        "def query_based_summarization(text, query):\n",
        "    relevant_sentences = retrieve_relevant_sentences(text, query)\n",
        "    summary = abstractive_summarization(relevant_sentences)\n",
        "    return summary\n",
        "\n",
        "print(f\"Query-based Summarization (church):\\n{query_based_summarization(cnn_example_text, 'church')}\\n\")\n",
        "print(f\"Query-based Summarization (hepatitis):\\n{query_based_summarization(cnn_example_text, 'hepatitis')}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tHGWZYxK1eW",
        "outputId": "323a08eb-c600-4ce4-d948-e0303ac7fed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query-based Summarization (church):\n",
            "A Roman Catholic bishop in North Dakota has been suspended amid an investigation into an outbreak of hepatitis A that may have exposed hundreds of church members.\n",
            "\n",
            "Query-based Summarization (hepatitis):\n",
            "The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand and Jamestown to the hepatitis A virus in late September and early October.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Metrics"
      ],
      "metadata": {
        "id": "1A__2AEymrns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define metrics"
      ],
      "metadata": {
        "id": "HLduTmIZBJSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install rouge_score\n",
        "!pip install bert_score\n",
        "!pip install textstat\n",
        "!pip install language_tool_python"
      ],
      "metadata": {
        "id": "ulsW0sQmoR9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textstat\n",
        "from rouge_score import rouge_scorer\n",
        "import spacy\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import language_tool_python\n",
        "import bert_score"
      ],
      "metadata": {
        "id": "N0Q7DnLl9Mqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize global variables\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "# Define functions to calculate various metrics\n",
        "def calculate_rouge_scores(summary, reference):\n",
        "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"], use_stemmer=True)\n",
        "    scores = scorer.score(summary, reference)\n",
        "    return scores\n",
        "\n",
        "def calculate_bert_score(summary, reference):\n",
        "    _, _, bert_score_f1 = bert_score.score([summary], [reference], verbose=False)\n",
        "    return bert_score_f1.item()\n",
        "\n",
        "def readability_flesch_score(summary):\n",
        "    return textstat.flesch_reading_ease(summary)\n",
        "\n",
        "def compression_rate(summary, original_text):\n",
        "    return len(summary) / len(original_text)\n",
        "\n",
        "def density(summary):\n",
        "    word_count = len(summary.split())\n",
        "    unique_word_count = len(set(summary.split()))\n",
        "    return unique_word_count / word_count\n",
        "\n",
        "def coverage(summary, original_text):\n",
        "    summary_words = set(summary.split())\n",
        "    original_words = set(original_text.split())\n",
        "    return len(summary_words.intersection(original_words)) / len(original_words)\n",
        "\n",
        "def calculate_bleu_score(summary, reference):\n",
        "    # Convert summary and reference to lists of tokens\n",
        "    summary_tokens = summary.split()\n",
        "    reference_tokens = reference.split()\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = sentence_bleu([reference_tokens], summary_tokens)\n",
        "    return bleu_score\n",
        "\n",
        "def calculate_entity_grid_score(summary):\n",
        "    try:\n",
        "      doc = nlp(summary)\n",
        "      entity_grid = {}\n",
        "\n",
        "      # Build entity grid\n",
        "      for ent in doc.ents:\n",
        "          if ent.label_ not in entity_grid:\n",
        "              entity_grid[ent.label_] = set()\n",
        "          entity_grid[ent.label_].add(ent.text.lower())\n",
        "\n",
        "      # Calculate entity grid score\n",
        "      entity_grid_score = 0\n",
        "      for label, entities in entity_grid.items():\n",
        "          if len(entities) > 1:\n",
        "              entity_grid_score += 1\n",
        "      entity_grid_score /= len(entity_grid)\n",
        "\n",
        "      return entity_grid_score\n",
        "\n",
        "    except:\n",
        "      return None\n",
        "\n",
        "def calculate_lexical_similarity(summary, original_text):\n",
        "    # Convert summary and original text to lists of strings\n",
        "    documents = [summary, original_text]\n",
        "\n",
        "    # Vectorize documents\n",
        "    vectorizer = CountVectorizer().fit_transform(documents)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_similarity = (vectorizer * vectorizer.T).toarray()[0, 1]\n",
        "\n",
        "    return cosine_similarity\n",
        "\n",
        "def calculate_jaccard_index(summary, original_text):\n",
        "    # Convert summary and original text to sets of words\n",
        "    summary_words = set(summary.split())\n",
        "    original_words = set(original_text.split())\n",
        "\n",
        "    # Calculate Jaccard Index\n",
        "    jaccard_index = len(summary_words.intersection(original_words)) / len(summary_words.union(original_words))\n",
        "\n",
        "    return jaccard_index\n",
        "\n",
        "def calculate_grammar_score(summary):\n",
        "    matches = tool.check(summary)\n",
        "    grammar_score = len(matches) / len(summary.split())\n",
        "\n",
        "    return grammar_score"
      ],
      "metadata": {
        "id": "wXU37qgWnH8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "3_y6CXiUBMUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "data = []\n",
        "\n",
        "for idx, article in cnn_daily[:10].iterrows():\n",
        "    start_time = time.time()\n",
        "    original_text = article[\"article\"]\n",
        "    reference_summary = article[\"summary\"]\n",
        "\n",
        "    # Create summaries\n",
        "\n",
        "    baseline_summary = baseline(original_text)\n",
        "    abstractive_summary = abstractive_summarization(original_text)\n",
        "    extractive_summary = extractive_summarization(original_text)\n",
        "    # hybrid_summary = hybrid_summarization(original_text)\n",
        "    # hybrid_importance_summary = hybrid_summarization_important_sentences(original_text)\n",
        "    graph_summary = graph_based_summarization(original_text)\n",
        "    ensemble_summary = ensemble_summarization(extractive_summary, abstractive_summary)\n",
        "    # iterative_summary = iterative_abstractive_summarization(original_text)\n",
        "\n",
        "\n",
        "    summaries = [\n",
        "        (\"0_Baseline\", baseline_summary),\n",
        "        (\"Abstractive\", abstractive_summary),\n",
        "        (\"Extractive\", extractive_summary),\n",
        "        # (\"Hybrid two-step\", hybrid_summary),\n",
        "        # (\"Hybrid importance\", hybrid_importance_summary),\n",
        "        (\"Graph\", graph_summary),\n",
        "        (\"Ensemble\", ensemble_summary)\n",
        "        # (\"Iterative\", iterative_summary)\n",
        "\n",
        "    ]\n",
        "\n",
        "    # Get metrics\n",
        "    for summary_type, summary in summaries:\n",
        "        row = {\n",
        "            \"Article Index\": idx,\n",
        "            \"Summary Type\": summary_type,\n",
        "            \"Summary\": summary,\n",
        "            \"F1 Accuracy (ROUGE2)\": calculate_rouge_scores(summary, reference_summary)[\"rouge2\"].fmeasure,\n",
        "            \"Readability (Flesch)\": readability_flesch_score(summary),\n",
        "            \"Entity Grid Score\": calculate_entity_grid_score(summary),\n",
        "            \"Lexical Similarity\": calculate_lexical_similarity(summary, original_text),\n",
        "            \"(Diversity) Jaccard Index\": calculate_jaccard_index(summary, original_text),\n",
        "            # \"Grammar Score\": calculate_grammar_score(summary),\n",
        "            \"Compression Rate\": compression_rate(summary, original_text),\n",
        "            \"Density\": density(summary),\n",
        "            \"Coverage\": coverage(summary, original_text)\n",
        "        }\n",
        "        data.append(row)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    # Print progress\n",
        "    print(f\"Processed article {idx+1} of {len(wikihow_clean)}, time taken: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "3S4NYnhp9YVK",
        "outputId": "5a6422ff-88bc-4eba-abbb-19d1db95ac25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed article 1 of 181925, time taken: 20.28 seconds\n",
            "Processed article 2 of 181925, time taken: 20.74 seconds\n",
            "Processed article 3 of 181925, time taken: 24.65 seconds\n",
            "Processed article 4 of 181925, time taken: 21.34 seconds\n",
            "Processed article 5 of 181925, time taken: 21.06 seconds\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-8468ade7260e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbaseline_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mabstractive_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabstractive_summarization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mextractive_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractive_summarization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# hybrid_summary = hybrid_summarization(original_text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-9a0255216a80>\u001b[0m in \u001b[0;36mabstractive_summarization\u001b[0;34m(text, max_length, min_length, do_sample, truncate, summarizer)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmin_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"summary_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \"\"\"\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \"\"\"\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         if (\n\u001b[1;32m    167\u001b[0m             \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             )\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1013\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text2text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"min_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m             )\n\u001b[1;32m   1523\u001b[0m             \u001b[0;31m# 13. run beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1524\u001b[0;31m             return self.beam_search(\n\u001b[0m\u001b[1;32m   1525\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2808\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2810\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2811\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2812\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1414\u001b[0m                 )\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1417\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1271\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1272\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 )\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1104\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/pegasus/modeling_pegasus.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import time\n",
        "import concurrent.futures\n",
        "\n",
        "data = []\n",
        "\n",
        "def process_article(idx, article):\n",
        "    start_time = time.time()\n",
        "    original_text = article[\"article\"]\n",
        "    reference_summary = article[\"summary\"]\n",
        "\n",
        "    # Create summaries\n",
        "    baseline_summary = baseline(original_text)\n",
        "    abstractive_summary = abstractive_summarization(original_text)\n",
        "    extractive_summary = extractive_summarization(original_text)\n",
        "    graph_summary = graph_based_summarization(original_text)\n",
        "    ensemble_summary = ensemble_summarization(extractive_summary, abstractive_summary)\n",
        "\n",
        "    summaries = [\n",
        "        (\"0_Baseline\", baseline_summary),\n",
        "        (\"Abstractive\", abstractive_summary),\n",
        "        (\"Extractive\", extractive_summary),\n",
        "        (\"Graph\", graph_summary),\n",
        "        (\"Ensemble\", ensemble_summary)\n",
        "    ]\n",
        "\n",
        "    local_data = []\n",
        "    # Get metrics\n",
        "    for summary_type, summary in summaries:\n",
        "        row = {\n",
        "            \"Article Index\": idx,\n",
        "            \"Summary Type\": summary_type,\n",
        "            \"Summary\": summary,\n",
        "            \"F1 Accuracy (ROUGE2)\": calculate_rouge_scores(summary, reference_summary)[\"rouge2\"].fmeasure,\n",
        "            \"Readability (Flesch)\": readability_flesch_score(summary),\n",
        "            \"Entity Grid Score\": calculate_entity_grid_score(summary),\n",
        "            \"Lexical Similarity\": calculate_lexical_similarity(summary, original_text),\n",
        "            \"(Diversity) Jaccard Index\": calculate_jaccard_index(summary, original_text),\n",
        "            \"Compression Rate\": compression_rate(summary, original_text),\n",
        "            \"Density\": density(summary),\n",
        "            \"Coverage\": coverage(summary, original_text)\n",
        "        }\n",
        "        local_data.append(row)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    # Print progress\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    print(f\"[{current_time}] Processed article {idx+1} of {len(cnn_daily[:10])}, time taken: {elapsed_time:.2f} seconds\")\n",
        "    # print(f\"Processed article {idx+1} of {len(cnn_daily[:10])}, time taken: {elapsed_time:.2f} seconds\")\n",
        "    return local_data\n",
        "\n",
        "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "    results = [executor.submit(process_article, idx, article) for idx, article in cnn_daily[:10].iterrows()]\n",
        "\n",
        "for future in concurrent.futures.as_completed(results):\n",
        "    data.extend(future.result())\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "t55vlGKePJ6E",
        "outputId": "6393158f-2e78-4e70-a23f-0cc9906be8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-f3f075a7925b>\u001b[0m in \u001b[0;36m<cell line: 49>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlocal_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcessPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_article\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcnn_daily\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/process.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor_manager_thread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executor_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m         \u001b[0;31m# To reduce the risk of opening too many files, remove references to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;31m# objects that use file descriptors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by \"Summary Type\" and calculate the mean of the metrics\n",
        "mean_metrics = df.groupby(\"Summary Type\").mean()\n",
        "\n",
        "# Reset index to move \"Summary Type\" back to a column\n",
        "mean_metrics.reset_index(inplace=True)\n",
        "\n",
        "# Display the aggregated metrics\n",
        "mean_metrics.drop(['Article Index'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "JSahbC3LAuJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_metrics"
      ],
      "metadata": {
        "id": "UBFtyW6WDAXu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}